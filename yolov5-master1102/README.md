### 基于yolov5的改进库(保证每周同步更新一次维护，不定期更新算法代码和引进结果实验！PS：注意层的使用大部分都适合少量数据集和特定业务目标数据集【其实一般的高精度较大模型魔改能提点也很正常，增大了参数量和模型复杂度，这里还要考虑实际情况的泛化问题，至于超大规模探索最优backbone这个其实会受制于硬件资源】

对于自注意力机制的使用：很多人与CNN相结合使用得到精度提升，个人理解：原因不仅仅是长距离的依赖，早期我们使用固定权重的滤波器提取边缘再到CNN，CNN也许是对应着高通滤波，而self-attention对应于低通滤波，那么相当于对featuremap进行了一次平滑，这样从某种程度上可以解释互补之后的提升；
而且transofromer是很难发生过拟合或者说不存在，其实实际操作中，这些改动没有质变，最实在的还是你训练数据集的拟合够不够好，你的模型是否能反映出数据之间的特征特异性。
